{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPow2G28G6g2/i9/FOx1KBK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanishika-c15/ml_assignments/blob/main/ass4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Q7XaQw6_fKuJ",
        "outputId": "218efffc-5ed7-4f8f-be07-48f14ed75edf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3239965512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# add bias column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mlearning_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "n = 500\n",
        "\n",
        "# base correlated variable\n",
        "x = np.random.rand(n)*10\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'X1': x,\n",
        "    'X2': x + np.random.randn(n)*0.2,\n",
        "    'X3': x*2 + np.random.randn(n)*0.5,\n",
        "    'X4': x*0.5 + np.random.randn(n)*0.1,\n",
        "    'X5': x + 5 + np.random.randn(n)*0.2,\n",
        "    'X6': x*3 + np.random.randn(n)*0.5,\n",
        "    'X7': 10 - x + np.random.randn(n)*0.3\n",
        "})\n",
        "\n",
        "# target variable with noise\n",
        "data['Y'] = 4*x + np.random.randn(n)*2\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def ridge_regression(X, y, lr, lam, epochs):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "    cost_list = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        y_pred = X.dot(w)\n",
        "        error = y_pred - y\n",
        "\n",
        "        grad = (2/m)*(X.T.dot(error) + lam*w)\n",
        "        w -= lr * grad\n",
        "\n",
        "        cost = (1/m)*(np.sum(error**2) + lam*np.sum(w**2))\n",
        "        cost_list.append(cost)\n",
        "    return w, cost_list\n",
        "    X = data.drop(columns=['Y']).values\n",
        "y = data['Y'].values\n",
        "\n",
        "# add bias column\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "learning_rates = [0.0001,0.001,0.01,0.1,1,10]\n",
        "lambdas = [1e-15,1e-10,1e-5,1e-3,0,1,10,20]\n",
        "\n",
        "best_score=-999\n",
        "best_config=None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w, cost = ridge_regression(X, y, lr, lam, 1000)\n",
        "        pred = X.dot(w)\n",
        "        score = r2_score(y, pred)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = (lr, lam)\n",
        "print(\"Best Learning Rate:\", best_config[0])\n",
        "print(\"Best Regularization Parameter:\", best_config[1])\n",
        "print(\"Best R2 Score:\", best_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Hitters.csv\")\n",
        "\n",
        "# Drop null rows\n",
        "df = df.dropna()\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df.drop(\"Salary\", axis=1)\n",
        "y = df[\"Salary\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "ridge = Ridge(alpha=0.5748)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "lasso = Lasso(alpha=0.5748)\n",
        "lasso.fit(X_train, y_train)\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "print(\"Linear:\", r2_score(y_test, lr.predict(X_test)))\n",
        "print(\"Ridge:\", r2_score(y_test, ridge.predict(X_test)))\n",
        "print(\"Lasso:\", r2_score(y_test, lasso.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "z3G3nfs0feUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "ridgecv = RidgeCV(alphas=[0.1,1,5,10,20,50,100])\n",
        "ridgecv.fit(X, y)\n",
        "\n",
        "print(\"Best alpha for Ridge:\", ridgecv.alpha_)\n",
        "lassocv = LassoCV(alphas=[0.1,1,5,10,20], cv=5)\n",
        "lassocv.fit(X, y)\n",
        "\n",
        "print(\"Best alpha for Lasso:\", lassocv.alpha_)\n"
      ],
      "metadata": {
        "id": "shqbwxNafyrg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}