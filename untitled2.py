# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RCe-Uq6JLAwAWsmgXi_Z2xZRwhxAukCg
"""

# Q1
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold, train_test_split

# Load dataset from Google Drive
file_id = '1O_NwpJT-8xGfU_-3llUl2sgPu0xllOrX'
url = f'https://drive.google.com/uc?export=download&id={file_id}'
try:
    df_house = pd.read_csv(url)
except Exception:
    df_house = pd.read_csv(url, header=None)

# Assume last column is price if column named differently
if 'price' not in df_house.columns:
    df_house.columns = list(df_house.columns[:-1]) + ['price']

X = df_house.drop(columns=['price']).values
y = df_house['price'].values.reshape(-1, 1)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kf = KFold(n_splits=5, shuffle=True, random_state=42)
betas = []
r2s = []
preds = []

for train_index, test_index in kf.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y[train_index], y[test_index]
    # add intercept
    Xb = np.hstack([np.ones((X_train.shape[0], 1)), X_train])
    beta = np.linalg.pinv(Xb.T @ Xb) @ Xb.T @ y_train
    betas.append(beta)
    Xb_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])
    y_pred = Xb_test @ beta
    preds.append(y_pred)
    r2s.append(r2_score(y_test, y_pred))

best_idx = int(np.argmax(r2s))
best_beta = betas[best_idx]

# Use best beta to predict after splitting 70/30
Xb_all = np.hstack([np.ones((X_scaled.shape[0], 1)), X_scaled])
X_train70, X_test30, y_train70, y_test30 = train_test_split(Xb_all, y, test_size=0.3, random_state=42)
# Note: X_train70 already has intercept column; fit not required if using best_beta
# Predict on test30 using best_beta (if dimensions match)
if X_train70.shape[1] == best_beta.shape[0]:
    y_pred_30 = X_test30 @ best_beta
    r2_30 = r2_score(y_test30, y_pred_30)
else:
    # fallback: compute beta on 70% via least squares
    beta_70 = np.linalg.pinv(X_train70.T @ X_train70) @ X_train70.T @ y_train70
    y_pred_30 = X_test30 @ beta_70
    r2_30 = r2_score(y_test30, y_pred_30)

print('Q1: 5-fold R2 scores:', r2s)
print('Q1: Best fold index:', best_idx)
print('Q1: R2 on 70/30 test using best beta:', r2_30)

# Q2
import math
from sklearn.model_selection import train_test_split

# Split into 56% train, 14% val, 30% test
X_full = X_scaled
y_full = y.flatten()
X_temp, X_test2, y_temp, y_test2 = train_test_split(X_full, y_full, test_size=0.30, random_state=1)
val_fraction = 0.14 / (0.56 + 0.14)
X_train2, X_val2, y_train2, y_val2 = train_test_split(X_temp, y_temp, test_size=val_fraction, random_state=1)

def gradient_descent(X, y, lr, iterations=1000):
    m, n = X.shape
    Xb = np.hstack([np.ones((m,1)), X])
    theta = np.zeros((n+1,))
    for i in range(iterations):
        preds = Xb @ theta
        error = preds - y
        grad = (1/m) * (Xb.T @ error)
        theta = theta - lr * grad
    return theta

lrs = [0.001, 0.01, 0.1, 1]
results = []
for lr in lrs:
    theta = gradient_descent(X_train2, y_train2, lr, iterations=1000)
    # validation
    Xv_b = np.hstack([np.ones((X_val2.shape[0],1)), X_val2])
    Xt_b = np.hstack([np.ones((X_test2.shape[0],1)), X_test2])
    y_val_pred = Xv_b @ theta
    y_test_pred = Xt_b @ theta
    r2_val = r2_score(y_val2, y_val_pred)
    r2_test = r2_score(y_test2, y_test_pred)
    results.append({'lr': lr, 'theta': theta, 'r2_val': r2_val, 'r2_test': r2_test})

best = max(results, key=lambda x: x['r2_val'])
print('\nQ2: Results for learning rates:')
for r in results:
    print(r)
print('\nQ2: Best by validation R2:', best['lr'], 'R2_val:', best['r2_val'], 'R2_test:', best['r2_test'])

# Q3
import requests
from io import StringIO
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression

url_autos = 'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'
cols = ["symboling", "normalized_losses", "make", "fuel_type", "aspiration","num_doors", "body_style", "drive_wheels",
        "engine_location", "wheel_base", "length", "width", "height", "curb_weight",
        "engine_type", "num_cylinders", "engine_size", "fuel_system", "bore", "stroke",
        "compression_ratio", "horsepower", "peak_rpm", "city_mpg", "highway_mpg", "price"]
resp = requests.get(url_autos)
df_auto = pd.read_csv(StringIO(resp.text), names=cols, na_values='?')

# Drop rows with price NaN
# Central tendency imputation for others
for c in df_auto.columns:
    if df_auto[c].dtype == object:
        if df_auto[c].isnull().any():
            df_auto[c].fillna(df_auto[c].mode()[0], inplace=True)
    else:
        if df_auto[c].isnull().any():
            df_auto[c].fillna(df_auto[c].median(), inplace=True)

df_auto = df_auto.dropna(subset=['price'])

# Convert num_doors and num_cylinders word numbers to digits
word_to_num = {
    'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,
    'seven':7,'eight':8,'nine':9,'fourteen':14
}

def w2n(val):
    try:
        return int(val)
    except Exception:
        val0 = str(val).lower()
        return word_to_num.get(val0, np.nan)

df_auto['num_doors'] = df_auto['num_doors'].apply(w2n)
df_auto['num_cylinders'] = df_auto['num_cylinders'].apply(w2n)

# Dummy encode body_style and drive_wheels
df_auto = pd.get_dummies(df_auto, columns=['body_style', 'drive_wheels'], drop_first=True)

# Label encode make, aspiration, engine_location, fuel_type
le = LabelEncoder()
for c in ['make', 'aspiration', 'engine_location', 'fuel_type']:
    df_auto[c] = le.fit_transform(df_auto[c].astype(str))

# fuel_system: contains 'pfi' ->1 else 0
df_auto['fuel_system'] = df_auto['fuel_system'].apply(lambda x: 1 if 'pfi' in str(x).lower() else 0)
# engine_type: contains 'ohc' ->1 else 0
df_auto['engine_type'] = df_auto['engine_type'].apply(lambda x: 1 if 'ohc' in str(x).lower() else 0)

# Prepare X and y
X_auto = df_auto.drop(columns=['price']).astype(float)
y_auto = df_auto['price'].astype(float)

scaler2 = StandardScaler()
X_auto_scaled = scaler2.fit_transform(X_auto)

X_trainA, X_testA, y_trainA, y_testA = train_test_split(X_auto_scaled, y_auto, test_size=0.3, random_state=42)

lr_model = LinearRegression()
lr_model.fit(X_trainA, y_trainA)
y_predA = lr_model.predict(X_testA)
print('\nQ3: R2 without PCA:', r2_score(y_testA, y_predA))

# PCA reduction (retain 95% variance)
pca = PCA(n_components=0.95, svd_solver='full')
X_reduced = pca.fit_transform(X_auto_scaled)
Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reduced, y_auto, test_size=0.3, random_state=42)

lr_model2 = LinearRegression()
lr_model2.fit(Xr_train, yr_train)
ypred_r = lr_model2.predict(Xr_test)
print('Q3: R2 with PCA:', r2_score(yr_test, ypred_r))

# Compare
print('Q3: PCA components retained:', pca.n_components_)